# Database Setup Guide

## Overview

This API uses PostgreSQL with Drizzle ORM for database management. The schema supports real-time incident tracking with video streams, AI-generated snapshots, and timeline events.

## Prerequisites

- PostgreSQL 14+ installed and running
- Database created (e.g., `emergency_copilot`)

## Environment Setup

1. Copy `.env.example` to `.env`:
   ```bash
   cp .env.example .env
   ```

2. Update `DATABASE_URL` in `.env` with your PostgreSQL connection string:
   ```
   DATABASE_URL=postgresql://username:password@localhost:5432/emergency_copilot
   ```

   For Supabase (IPv6 workaround), also set:
   ```
   DATABASE_POOLER_URL=postgresql://...pooler.supabase.com:6543/postgres
   ```

## Database Schema

### Tables

#### `incidents`
Represents grouped emergency events at a location. Multiple videos from the same area are grouped into one incident.

| Column | Type | Description |
|--------|------|-------------|
| `id` | UUID | Primary key |
| `status` | VARCHAR | 'active', 'resolved', 'archived' |
| `lat` | REAL | Latitude coordinate |
| `lng` | REAL | Longitude coordinate |
| `startedAt` | TIMESTAMP | When first video in incident started |
| `createdAt` | TIMESTAMP | Record creation time |
| `updatedAt` | TIMESTAMP | Last update time |

#### `videos`
Individual video streams from callers. One incident can have multiple videos.

| Column | Type | Description |
|--------|------|-------------|
| `id` | UUID | Primary key |
| `incidentId` | UUID | Foreign key to incidents (nullable) |
| `status` | VARCHAR | 'live', 'ended', 'recorded' |
| `currentState` | TEXT | AI-generated human-readable summary |
| `videoUrl` | VARCHAR | URL to recorded video file (nullable) |
| `lat` | REAL | Latitude coordinate |
| `lng` | REAL | Longitude coordinate |
| `startedAt` | TIMESTAMP | When stream started |
| `endedAt` | TIMESTAMP | When stream ended (nullable) |
| `createdAt` | TIMESTAMP | Record creation time |
| `updatedAt` | TIMESTAMP | Last update time |

#### `snapshots`
Raw observations from Overshoot video analysis. These are buffered and processed in batches.

| Column | Type | Description |
|--------|------|-------------|
| `id` | UUID | Primary key |
| `videoId` | UUID | Foreign key to videos (cascade delete) |
| `timestamp` | TIMESTAMP | When observation occurred |
| `lat` | REAL | Latitude coordinate |
| `lng` | REAL | Longitude coordinate |
| `type` | VARCHAR | Snapshot type (e.g., 'overshoot_analysis') |
| `scenario` | VARCHAR | Classification (vehicle_accident, fire, etc.) |
| `data` | JSONB | Structured analysis data from Overshoot |
| `createdAt` | TIMESTAMP | Record creation time |
| `updatedAt` | TIMESTAMP | Last update time |

#### `timeline_events`
AI-derived meaningful events generated by analyzing snapshot batches with Gemini.

| Column | Type | Description |
|--------|------|-------------|
| `id` | UUID | Primary key |
| `videoId` | UUID | Foreign key to videos (cascade delete) |
| `timestamp` | TIMESTAMP | When event occurred |
| `description` | TEXT | Human-readable event description |
| `fromState` | JSONB | Previous state context |
| `toState` | JSONB | New state context |
| `confidence` | REAL | AI confidence score (0-1) |
| `sourceSnapshots` | JSONB | Array of snapshot IDs used for this event |
| `createdAt` | TIMESTAMP | Record creation time |

### Relationships

```
incidents (1) ──────────< videos (many)
                              │
                              ├──────< snapshots (many)
                              │
                              └──────< timeline_events (many)
```

- One **incident** can have many **videos** (multiple callers at same location)
- One **video** can have many **snapshots** (continuous analysis results)
- One **video** can have many **timeline_events** (AI-generated insights)
- Deleting a video cascades to delete its snapshots and timeline_events

### Status Values

**Incident Status:**
- `active` - Ongoing emergency
- `resolved` - Emergency handled
- `archived` - Historical record

**Video Status:**
- `live` - Currently streaming
- `ended` - Stream stopped, no recording yet
- `recorded` - Recording available at videoUrl

**Snapshot Scenario:**
- `vehicle_accident` - Vehicle collision
- `fire` - Fire or smoke
- `injury` / `medical` - Visible injury
- `weapon` - Weapon detected
- `person_down` - Person on ground
- `crowd` - Large gathering
- `scene_analysis` - General description
- `unknown` - Unclassified

## Migration Commands

### Generate Migration Files
Creates SQL migration files based on schema changes:
```bash
bun run db:generate
```

### Apply Migrations
Run pending migrations against your database:
```bash
bun run db:migrate
```

### Push Schema (Development)
Push schema changes directly without migration files (for rapid development):
```bash
bun run db:push
```

### Drizzle Studio
Open visual database browser:
```bash
bun run db:studio
```

## Quick Start

1. Ensure PostgreSQL is running
2. Create database: `createdb emergency_copilot`
3. Set `DATABASE_URL` in `.env`
4. Generate and run migrations:
   ```bash
   bun run db:generate
   bun run db:push
   ```
5. Start the API:
   ```bash
   bun run dev
   ```

## Development Workflow

1. **Make schema changes** in `src/models/index.ts`
2. **Generate migrations**: `bun run db:generate`
3. **Review** generated SQL in `drizzle/` folder
4. **Apply migrations**: `bun run db:push` (dev) or `bun run db:migrate` (prod)

## Production Considerations

- Always use `db:migrate` instead of `db:push` in production
- Backup database before running migrations
- Test migrations in staging environment first
- Keep migration files in version control
- Use connection pooling for production deployments
